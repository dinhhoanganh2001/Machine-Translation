# -*- coding: utf-8 -*-
"""DEMO(vie-to-eng).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-gnh-3-KY9XEdM1W_91j0SkGvgyP-Bl
"""

from flask import Flask, render_template, request, session, url_for, redirect, flash
#from flask_ngrok import run_with_ngrok
import pathlib
import random
import string
import re
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import TextVectorization
from numpy.random import seed

seed(1)# keras seed fixing import
tf.random.set_seed(2)
physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass
"""## Dat"""

text_file = pathlib.Path('dataset/eng-fre.txt')
with open(text_file) as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    vie, eng = line.split("\t")
    text_pairs.append((vie, eng))
# for _ in range(5):
#     print(random.choice(text_pairs))
num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]
en_count = []
fr_count = []
for line in lines:
  en, fr = line.split("\t")
  for word in en.split(" "):
    en_count.append(word)
  for word in fr.split(" "):
    fr_count.append(word)
strip_chars = string.punctuation
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

vocab_size_en = len(set(en_count))
vocab_size_fr = len(set(fr_count)) 
sequence_length = 40
batch_size = 32
#print(vocab_size_en)
#print(vocab_size_fr)

# dung chung
def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(lowercase, "[%s]" % re.escape(strip_chars), "")

en_vectorization_dat = TextVectorization(
    max_tokens=vocab_size_en,
    output_mode="int",
    output_sequence_length=sequence_length,
)
fr_vectorization_dat = TextVectorization(
    max_tokens=vocab_size_fr,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    standardize=custom_standardization,
)
train_en_texts = [pair[0] for pair in train_pairs]
train_fr_texts = [pair[1] for pair in train_pairs]
en_vectorization_dat.adapt(train_en_texts)
fr_vectorization_dat.adapt(train_fr_texts)

# dun ch
class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim)
        self.dense_proj = keras.Sequential(
            [layers.Dense(dense_dim, activation="relu"),
             layers.Dense(embed_dim), ]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()

    def call(self, inputs, mask=None):
        if mask is not None:
            mask = mask[:, tf.newaxis, :]
        attention_output = self.attention(
            inputs, inputs, attention_mask=mask)
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "num_heads": self.num_heads,
            "dense_dim": self.dense_dim,
        })
        return config

class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=input_dim, output_dim=output_dim)
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=output_dim)
        self.sequence_length = sequence_length
        self.input_dim = input_dim
        self.output_dim = output_dim

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)

    def get_config(self):
        config = super().get_config()
        config.update({
            "output_dim": self.output_dim,
            "sequence_length": self.sequence_length,
            "input_dim": self.input_dim,
        })
        return config

class TransformerDecoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = latent_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(latent_dim, activation="relu"), layers.Dense(embed_dim), ]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)
        if mask is not None:
            padding_mask = tf.cast(
                mask[:, tf.newaxis, :], dtype="int32")
            padding_mask = tf.minimum(padding_mask, causal_mask)
        attention_output_1 = self.attention_1(
            query=inputs,
            value=inputs,
            key=inputs,
            attention_mask=causal_mask)
        attention_output_1 = self.layernorm_1(inputs + attention_output_1)
        attention_output_2 = self.attention_2(
            query=attention_output_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
        )
        attention_output_2 = self.layernorm_2(
            attention_output_1 + attention_output_2)
        proj_output = self.dense_proj(attention_output_2)
        return self.layernorm_3(attention_output_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "num_heads": self.num_heads,
            "dense_dim": self.dense_dim,
        })
        return config

embed_dim = 256
latent_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_en, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_en, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_fr, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer_dat = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

fr_vocab = fr_vectorization_dat.get_vocabulary()
fr_index_lookup = dict(zip(range(len(fr_vocab)), fr_vocab))
max_decoded_sentence_length = 20
transformer_dat.load_weights('pretrain_weight(eng-fra)/checkpoint1')
def decode_sequence_dat(input_sentence):
    tokenized_input_sentence = en_vectorization_dat([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = fr_vectorization_dat([decoded_sentence])[:, :-1]
        predictions = transformer_dat([tokenized_input_sentence, tokenized_target_sentence])

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = fr_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence

# transformer_dat.summary()
# transformer_dat.compile(
#     "adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
# )

#transformer_dat.fit(train_ds, epochs=5, validation_data=val_ds)

#transformer_dat.save_weights('/content/drive/MyDrive/MachineTranslation/pretrain_weight(eng-fra)/checkpoint1')

"""## Hung

"""

text_file = pathlib.Path('dataset/vie-eng.txt')
with open(text_file) as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    vie, eng = line.split("\t")
    text_pairs.append((vie, eng))
# for _ in range(5):
#     print(random.choice(text_pairs))
num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]
en_count = []
vi_count = []
for line in lines:
  vie, en = line.split("\t")
  for word in vie.split(" "):
    vi_count.append(word)
  for word in en.split(" "):
    en_count.append(word)
strip_chars = string.punctuation
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

vocab_size_en = len(set(en_count))
vocab_size_vi = len(set(vi_count)) 
sequence_length = 40
batch_size = 32
# print(vocab_size_en)
# print(vocab_size_vi)


vie_vectorization = TextVectorization(
    max_tokens=vocab_size_vi,
    output_mode="int",
    output_sequence_length=sequence_length,
)
eng_vectorization = TextVectorization(
    max_tokens=vocab_size_en,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    standardize=custom_standardization,
)
train_vie_texts = [pair[0] for pair in train_pairs]
train_eng_texts = [pair[1] for pair in train_pairs]
vie_vectorization.adapt(train_vie_texts)
eng_vectorization.adapt(train_eng_texts)



embed_dim = 256
latent_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_vi, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_vi, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_en, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

eng_vocab = eng_vectorization.get_vocabulary()
eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))
max_decoded_sentence_length = 20
transformer.load_weights('pretrain_weight(vie-eng)/checkpoint1')
def decode_sequence(input_sentence):
    tokenized_input_sentence = vie_vectorization([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = eng_vectorization([decoded_sentence])[:, :-1]
        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = eng_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence

# print("Please enter your sentence: ")
# str = input()
#
# str_translated = decode_sequence(str)
# print("\n------>", str_translated)

"""
%%%%%%%%%%%%%%%%%%%%%%HOANG ANH%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"""

text_file = pathlib.Path('dataset/eng-vie.txt')
with open(text_file) as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    eng, vie = line.split("\t")
    vie = "[start] " + vie + " [end]"
    text_pairs.append((eng, vie))
# for _ in range(5):
#     print(random.choice(text_pairs))

num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]

# print(f"{len(text_pairs)} total pairs")
# print(f"{len(train_pairs)} training pairs")
# print(f"{len(val_pairs)} validation pairs")
# print(f"{len(test_pairs)} test pairs")

en_count = []
vi_count = []
for line in lines:
  eng, vie = line.split("\t")
  for word in eng.split(" "):
    en_count.append(word)
  for word in vie.split(" "):
    vi_count.append(word)

strip_chars = string.punctuation
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

vocab_size_en = len(set(en_count))
vocab_size_vi = len(set(vi_count)) 
sequence_length = 40
batch_size = 32
# print(vocab_size_en)
# print(vocab_size_vi)



eng_vectorization_ha = TextVectorization(
    max_tokens=vocab_size_en, 
    output_mode="int", 
    output_sequence_length=sequence_length,
)
vie_vectorization_ha = TextVectorization(
    max_tokens=vocab_size_vi,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    standardize=custom_standardization,
)
train_eng_texts = [pair[0] for pair in train_pairs]
train_vie_texts = [pair[1] for pair in train_pairs]
eng_vectorization_ha.adapt(train_eng_texts)
vie_vectorization_ha.adapt(train_vie_texts)



embed_dim = 256
latent_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_en, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_en, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_vi, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer_ha = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

transformer_ha.load_weights('pretrain_weight(eng-vie)/checkpoint1')

vie_vocab = vie_vectorization_ha.get_vocabulary()
vie_index_lookup = dict(zip(range(len(vie_vocab)), vie_vocab))
max_decoded_sentence_length = 20


def decode_sequence_ha(input_sentence):
    tokenized_input_sentence = eng_vectorization_ha([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = vie_vectorization_ha([decoded_sentence])[:, :-1]
        predictions = transformer_ha([tokenized_input_sentence, tokenized_target_sentence])

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = vie_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence

# print("Please enter your sentence: ")
# str = input()

# str_translated = decode_sequence_ha(str)
# print("\n------>", str_translated)

"""%%%%%%%%%%%%%%%%%%%%END HOANG ANH%%%%%%%%%%%%%%%%%%%%%%%%

## Michael
"""

text_file = pathlib.Path('dataset/fra.txt')

with open(text_file) as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    eng, fra, des = line.split("\t")
    eng = "[start] " + eng + " [end]"
    text_pairs.append((fra, eng))
# for _ in range(5):
#     print(random.choice(text_pairs))

num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]

# print(f"{len(text_pairs)} total pairs")
# print(f"{len(train_pairs)} training pairs")
# print(f"{len(val_pairs)} validation pairs")
# print(f"{len(test_pairs)} test pairs")

fra_count = []
eng_count = []
for line in lines:
  eng, fra, des = line.split("\t")
  for word in eng.split(" "):
    eng_count.append(word)
  for word in fra.split(" "):
    fra_count.append(word)

strip_chars = string.punctuation
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

vocab_size_en = len(set(eng_count))
vocab_size_fra = len(set(fra_count)) 
sequence_length = 40
batch_size = 32




fra_vectorization_michael = TextVectorization(
    max_tokens=vocab_size_fra, 
    output_mode="int", 
    output_sequence_length=sequence_length,
)
eng_vectorization_michael = TextVectorization(
    max_tokens=vocab_size_en,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    standardize=custom_standardization,
)
train_fra_texts = [pair[0] for pair in train_pairs]
train_eng_texts = [pair[1] for pair in train_pairs]
eng_vectorization_michael.adapt(train_eng_texts)
fra_vectorization_michael.adapt(train_fra_texts)



embed_dim = 256
latent_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_fra, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_fra, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_en, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer_michael = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

transformer_michael.load_weights('pretrain_weight(fra-eng)/model7.hdf5')

eng_vocab_michael = eng_vectorization_michael.get_vocabulary()
eng_index_lookup_michael = dict(zip(range(len(eng_vocab_michael)), eng_vocab_michael))
max_decoded_sentence_length = 20


def decode_sequence_michael(input_sentence):
    tokenized_input_sentence = fra_vectorization_michael([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = eng_vectorization_michael([decoded_sentence])[:, :-1]
        predictions = transformer_michael([tokenized_input_sentence, tokenized_target_sentence])

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = eng_index_lookup_michael[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence

# test_fra_texts = [pair[0] for pair in test_pairs]
# for _ in range(30):
#     input_sentence = random.choice(test_fra_texts)
#     translated = decode_sequence_michael(input_sentence)
#     print(input_sentence, "---->", translated)

# print("Please enter your sentence: ")
# str = input()

# str_translated = decode_sequence_michael(str)
# print("\n------>", str_translated)

# epochs = 5 # This should be at least 30 for convergence

# transformer_michael.summary()
# transformer_michael.compile(
#     "adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
# )
# transformer_michael.fit(train_ds, epochs=epochs, validation_data=val_ds)

"""## Quang"""

TEX_FILE = "dataset/eng-ita.txt"

with open(TEX_FILE) as f:
    lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
    firstLanguage, secondLanguage = line.split("\t")[0:2]
    secondLanguage = "[start] " + secondLanguage + " [end]"
    text_pairs.append((firstLanguage, secondLanguage))
# for _ in range(5):
#     print(random.choice(text_pairs))

num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]

firstL_count = []
secondL_count = []
for line in lines:
  firstL, secondL = line.split("\t")[0:2]
  for word in firstL.split(" "):
    firstL_count.append(word)
  for word in secondL.split(" "):
    secondL_count.append(word)
strip_chars = string.punctuation
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

vocab_size_firstL = len(set(firstL_count))
vocab_size_secondL = len(set(secondL_count)) 
sequence_length = 40
batch_size = 32





firstL_vectorization = TextVectorization(
    max_tokens=vocab_size_firstL, 
    output_mode="int",
    output_sequence_length=sequence_length,
)
secondL_vectorization = TextVectorization(
    max_tokens=vocab_size_secondL,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    standardize=custom_standardization,
)
train_first_language_texts = [pair[0] for pair in train_pairs]
train_second_language_texts = [pair[1] for pair in train_pairs]
firstL_vectorization.adapt(train_first_language_texts)
secondL_vectorization.adapt(train_second_language_texts)

embed_dim = 256
latent_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_firstL, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size_firstL, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size_secondL, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer_quang = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

transformer_quang.load_weights('pretrain_weight(eng-ita)/model_checkpoint.h5')

secondL_vocab = secondL_vectorization.get_vocabulary()
secondL_index_lookup = dict(zip(range(len(secondL_vocab)),secondL_vocab))
max_decoded_sentence_length = 20


def decode_sequence_quang(input_sentence):
    tokenized_input_sentence = firstL_vectorization([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = secondL_vectorization([decoded_sentence])[:, :-1]
        predictions = transformer_quang([tokenized_input_sentence, tokenized_target_sentence])
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = secondL_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence


# test_firstL_texts = [pair[0] for pair in test_pairs]
# for _ in range(30):
#     input_sentence = random.choice(test_firstL_texts)
#     translated = decode_sequence_quang(input_sentence)
#     print(input_sentence, "---->", translated)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/MachineTranslation

# # Commented out IPython magic to ensure Python compatibility.
# !mkdir -p /drive/ngrok-ssh
# # %cd /drive/ngrok-ssh
# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip
# !unzip -u ngrok-stable-linux-amd64.zip
# !cp /drive/ngrok-ssh/ngrok /ngrok
# !chmod +x /ngrok
# !/ngrok authtoken 223TtGTkYYtLUQHgoddvHzSYcED_5VjRdRZDfVPpzUNSh6gD3

# run_with_ngrok(app) 
# transformer.load_weights('pretrain_weight(vie-eng)/checkpoint1')
# transformer_ha.load_weights('pretrain_weight(eng-vie)/checkpoint1')
# transformer_dat.load_weights('pretrain_weight(eng-fra)/checkpoint1')
# transformer_michael.load_weights('pretrain_weight(fra-eng)/model2.hdf5')
# model.load_weights('pretrain_weight(eng-ita)/model_checkpoint.h5')

app = Flask(__name__)

@app.route('/load_vie_eng', methods=['GET', 'POST'])
def load_vie_eng():
    s = "Source sentence"
    translated = "Output sentence"
    return render_template('home.html', translated = translated,
        s = s, mode = 'vie-eng')

@app.route('/load_eng_vie', methods=['GET', 'POST'])
def load_eng_vie():
    s = "Source sentence"
    translated = "Output sentence"
    return render_template('home.html', translated = translated,
        s = s, mode = 'eng-vie')

@app.route('/load_eng_fre', methods=['GET', 'POST'])
def load_eng_fre():
    s = "Source sentence"
    translated = "Output sentence"
    return render_template('home.html', translated = translated,
        s = s, mode = 'eng-fre')

@app.route('/load_fre_eng', methods=['GET', 'POST'])
def load_fre_eng():
    s = "Source sentence"
    translated = "Output sentence"
    return render_template('home.html', translated = translated,
        s = s, mode = 'fre_eng')

@app.route('/load_eng_ita', methods=['GET', 'POST'])
def load_eng_ita():
    s = "Source sentence"
    translated = "Output sentence"
    return render_template('home.html', translated = translated,
        s = s, mode = 'eng-ita')
    

@app.route('/translate', methods=['GET', 'POST'])
def translate():
    mode = request.args.get("mode")
    s = request.args.get("query")
    if s is None:
        abort(400)
    print(s)
    if mode == 'vie-eng':
        translated = decode_sequence(s)
        translated = translated.replace('[start]', '')
        translated = translated.replace('[end]', '')
    elif mode == 'eng-vie':
        translated = decode_sequence_ha(s)
        translated = translated.replace('[start]', '')
        translated = translated.replace('[end]', '')
    elif mode == 'fre-eng':
        translated = decode_sequence_michael(s)
        translated = translated.replace('[start]', '')
        translated = translated.replace('[end]', '')
    elif mode == 'eng-fre':
        translated = decode_sequence_dat(s)
        translated = translated.replace('[start]', '')
        translated = translated.replace('[end]', '')
    elif mode == 'eng-ita':
        translated = decode_sequence_quang(s)
        translated = translated.replace('[start]', '')
        translated = translated.replace('[end]', '')
    else:
        translated = 'You have not chosen any modes'
        s = ''
    return render_template(
        "home.html",
        translated = translated,
        s = s,
        mode = mode,
    )

@app.route('/')
def home():
    s = "Source sentence"
    translated = "Output sentence"  
    return render_template('home.html', s= s, translated = translated)


if __name__ == "__main__":
    app.run(debug=True)
